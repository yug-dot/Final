{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f5064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NLP EXAM - COMPLETE READY-TO-USE CODE FOR ALL 3 QUESTIONS\n",
    "Copy the entire question code and run in Jupyter Notebook\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# QUESTION 1: TEXT PREPROCESSING & REPRESENTATION (15 MARKS)\n",
    "# Task: 1. Preprocess text\n",
    "#       2. Representation of text and comparison of outputs\n",
    "# ============================================================================\n",
    "\n",
    "# -------- FIRST RUN THIS CELL (IMPORTS & SETUP) --------\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download NLTK data (run once)\n",
    "nltk_data = ['punkt', 'stopwords', 'wordnet', 'omw-1.4']\n",
    "for item in nltk_data:\n",
    "    try:\n",
    "        nltk.download(item, quiet=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"✓ Setup Complete for Question 1!\")\n",
    "\n",
    "# -------- THEN RUN THIS CELL (MAIN CODE) --------\n",
    "\n",
    "# Sample input text (replace with exam question text)\n",
    "text1 = \"\"\"Natural Language Processing is an exciting field of artificial intelligence. \n",
    "It helps computers understand and process human language. NLP has many applications \n",
    "in real world scenarios.\"\"\"\n",
    "\n",
    "text2 = \"\"\"Artificial Intelligence and Machine Learning are transforming technology.\n",
    "These fields enable computers to learn and make intelligent decisions.\"\"\"\n",
    "\n",
    "# ===== PART 1: TEXT PREPROCESSING (5 marks) =====\n",
    "print(\"=\"*70)\n",
    "print(\"PART 1: TEXT PREPROCESSING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "    \n",
    "    # Step 1: Tokenization\n",
    "    print(f\"\\n1. Original Text:\\n{text}\\n\")\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    print(f\"2. After Tokenization ({len(tokens)} tokens):\\n{tokens[:10]}...\\n\")\n",
    "    \n",
    "    # Step 2: Remove punctuation\n",
    "    tokens_clean = [word for word in tokens if word.isalnum()]\n",
    "    print(f\"3. After Removing Punctuation ({len(tokens_clean)} tokens):\\n{tokens_clean[:10]}...\\n\")\n",
    "    \n",
    "    # Step 3: Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens_no_stop = [word for word in tokens_clean if word not in stop_words]\n",
    "    print(f\"4. After Removing Stopwords ({len(tokens_no_stop)} tokens):\\n{tokens_no_stop[:10]}...\\n\")\n",
    "    \n",
    "    # Step 4: Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed = [stemmer.stem(word) for word in tokens_no_stop]\n",
    "    print(f\"5. After Stemming:\\n{stemmed[:10]}...\\n\")\n",
    "    \n",
    "    # Step 5: Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens_no_stop]\n",
    "    print(f\"6. After Lemmatization:\\n{lemmatized[:10]}...\\n\")\n",
    "    \n",
    "    return tokens_clean, tokens_no_stop, lemmatized\n",
    "\n",
    "# Preprocess both texts\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREPROCESSING TEXT 1:\")\n",
    "print(\"=\"*70)\n",
    "tokens1_clean, tokens1_no_stop, tokens1_final = preprocess_text(text1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREPROCESSING TEXT 2:\")\n",
    "print(\"=\"*70)\n",
    "tokens2_clean, tokens2_no_stop, tokens2_final = preprocess_text(text2)\n",
    "\n",
    "# ===== PART 2: TEXT REPRESENTATION (5 marks) =====\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART 2: TEXT REPRESENTATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Representation 1: Bag of Words (BoW)\n",
    "print(\"\\n--- Bag of Words Representation ---\")\n",
    "bow1 = Counter(tokens1_final)\n",
    "bow2 = Counter(tokens2_final)\n",
    "print(f\"\\nText 1 BoW:\\n{dict(bow1)}\")\n",
    "print(f\"\\nText 2 BoW:\\n{dict(bow2)}\")\n",
    "\n",
    "# Representation 2: Term Frequency (TF)\n",
    "print(\"\\n--- Term Frequency Representation ---\")\n",
    "def calculate_tf(tokens):\n",
    "    bow = Counter(tokens)\n",
    "    total_words = len(tokens)\n",
    "    tf = {word: count/total_words for word, count in bow.items()}\n",
    "    return tf\n",
    "\n",
    "tf1 = calculate_tf(tokens1_final)\n",
    "tf2 = calculate_tf(tokens2_final)\n",
    "print(f\"\\nText 1 TF (top 5):\")\n",
    "for word, freq in sorted(tf1.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "    print(f\"  {word}: {freq:.4f}\")\n",
    "print(f\"\\nText 2 TF (top 5):\")\n",
    "for word, freq in sorted(tf2.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "    print(f\"  {word}: {freq:.4f}\")\n",
    "\n",
    "# Representation 3: TF-IDF\n",
    "print(\"\\n--- TF-IDF Representation ---\")\n",
    "def calculate_tfidf(documents):\n",
    "    \"\"\"Calculate TF-IDF for multiple documents\"\"\"\n",
    "    n_docs = len(documents)\n",
    "    \n",
    "    # Calculate Document Frequency (DF)\n",
    "    df = {}\n",
    "    for doc in documents:\n",
    "        unique_words = set(doc)\n",
    "        for word in unique_words:\n",
    "            df[word] = df.get(word, 0) + 1\n",
    "    \n",
    "    # Calculate IDF\n",
    "    idf = {word: np.log(n_docs / freq) for word, freq in df.items()}\n",
    "    \n",
    "    # Calculate TF-IDF for each document\n",
    "    tfidf_docs = []\n",
    "    for doc in documents:\n",
    "        tf = calculate_tf(doc)\n",
    "        tfidf = {word: tf_val * idf[word] for word, tf_val in tf.items()}\n",
    "        tfidf_docs.append(tfidf)\n",
    "    \n",
    "    return tfidf_docs, idf\n",
    "\n",
    "documents = [tokens1_final, tokens2_final]\n",
    "tfidf_results, idf_values = calculate_tfidf(documents)\n",
    "\n",
    "print(f\"\\nText 1 TF-IDF (top 5):\")\n",
    "for word, score in sorted(tfidf_results[0].items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "    print(f\"  {word}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nText 2 TF-IDF (top 5):\")\n",
    "for word, score in sorted(tfidf_results[1].items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "    print(f\"  {word}: {score:.4f}\")\n",
    "\n",
    "# ===== PART 3: COMPARISON OF REPRESENTATIONS (5 marks) =====\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART 3: COMPARISON OF REPRESENTATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cosine Similarity\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "    # Get common words\n",
    "    all_words = set(vec1.keys()) | set(vec2.keys())\n",
    "    \n",
    "    # Create vectors\n",
    "    v1 = [vec1.get(word, 0) for word in all_words]\n",
    "    v2 = [vec2.get(word, 0) for word in all_words]\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    dot_product = sum(a * b for a, b in zip(v1, v2))\n",
    "    magnitude1 = np.sqrt(sum(a**2 for a in v1))\n",
    "    magnitude2 = np.sqrt(sum(b**2 for b in v2))\n",
    "    \n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "# Compare using different representations\n",
    "sim_bow = cosine_similarity(dict(bow1), dict(bow2))\n",
    "sim_tf = cosine_similarity(tf1, tf2)\n",
    "sim_tfidf = cosine_similarity(tfidf_results[0], tfidf_results[1])\n",
    "\n",
    "print(\"\\nSimilarity between Text 1 and Text 2:\")\n",
    "print(f\"  Using Bag of Words:  {sim_bow:.4f}\")\n",
    "print(f\"  Using TF:            {sim_tf:.4f}\")\n",
    "print(f\"  Using TF-IDF:        {sim_tfidf:.4f}\")\n",
    "\n",
    "print(\"\\nComparison Summary:\")\n",
    "print(\"- Bag of Words: Simple word counts, doesn't consider document length\")\n",
    "print(\"- TF: Normalized by document length, better for different sized texts\")\n",
    "print(\"- TF-IDF: Highlights important words unique to documents\")\n",
    "print(f\"\\n✓ Question 1 Complete!\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# QUESTION 2: SEMANTIC UNDERSTANDING & LANGUAGE MODELING (15 MARKS)\n",
    "# Task: 1. Extraction of synonyms, antonyms, and hypernyms\n",
    "#       2. N-Gram Language Model with Laplace smoothing for prediction\n",
    "# ============================================================================\n",
    "\n",
    "# -------- FIRST RUN THIS CELL (IMPORTS & SETUP) --------\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "print(\"✓ Setup Complete for Question 2!\")\n",
    "\n",
    "# -------- THEN RUN THIS CELL (MAIN CODE) --------\n",
    "\n",
    "# ===== PART 1: SEMANTIC ANALYSIS WITH WORDNET (5 marks) =====\n",
    "print(\"=\"*70)\n",
    "print(\"PART 1: SEMANTIC ANALYSIS - SYNONYMS, ANTONYMS, HYPERNYMS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def get_synonyms(word):\n",
    "    \"\"\"Extract synonyms from WordNet\"\"\"\n",
    "    synonyms = set()\n",
    "    for synset in wordnet.synsets(word):\n",
    "        for lemma in synset.lemmas():\n",
    "            synonym = lemma.name().replace('_', ' ')\n",
    "            if synonym.lower() != word.lower():\n",
    "                synonyms.add(synonym)\n",
    "    return list(synonyms)\n",
    "\n",
    "def get_antonyms(word):\n",
    "    \"\"\"Extract antonyms from WordNet\"\"\"\n",
    "    antonyms = set()\n",
    "    for synset in wordnet.synsets(word):\n",
    "        for lemma in synset.lemmas():\n",
    "            if lemma.antonyms():\n",
    "                antonym = lemma.antonyms()[0].name().replace('_', ' ')\n",
    "                antonyms.add(antonym)\n",
    "    return list(antonyms)\n",
    "\n",
    "def get_hypernyms(word):\n",
    "    \"\"\"Extract hypernyms (more general terms) from WordNet\"\"\"\n",
    "    hypernyms = set()\n",
    "    for synset in wordnet.synsets(word):\n",
    "        for hypernym in synset.hypernyms():\n",
    "            for lemma in hypernym.lemmas():\n",
    "                hypernym_word = lemma.name().replace('_', ' ')\n",
    "                hypernyms.add(hypernym_word)\n",
    "    return list(hypernyms)\n",
    "\n",
    "def get_hyponyms(word):\n",
    "    \"\"\"Extract hyponyms (more specific terms) from WordNet\"\"\"\n",
    "    hyponyms = set()\n",
    "    for synset in wordnet.synsets(word):\n",
    "        for hyponym in synset.hyponyms():\n",
    "            for lemma in hyponym.lemmas():\n",
    "                hyponym_word = lemma.name().replace('_', ' ')\n",
    "                hyponyms.add(hyponym_word)\n",
    "    return list(hyponyms)\n",
    "\n",
    "# Test words (replace with exam question words)\n",
    "test_words = ['good', 'happy', 'dog', 'car']\n",
    "\n",
    "for word in test_words:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Word: '{word}'\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    synonyms = get_synonyms(word)\n",
    "    antonyms = get_antonyms(word)\n",
    "    hypernyms = get_hypernyms(word)\n",
    "    hyponyms = get_hyponyms(word)\n",
    "    \n",
    "    print(f\"\\nSynonyms ({len(synonyms)}): {synonyms[:5] if synonyms else 'None found'}\")\n",
    "    print(f\"Antonyms ({len(antonyms)}): {antonyms[:5] if antonyms else 'None found'}\")\n",
    "    print(f\"Hypernyms ({len(hypernyms)}): {hypernyms[:5] if hypernyms else 'None found'}\")\n",
    "    print(f\"Hyponyms ({len(hyponyms)}): {hyponyms[:3] if hyponyms else 'None found'}\")\n",
    "    \n",
    "    # Show definitions\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if synsets:\n",
    "        print(f\"\\nDefinitions:\")\n",
    "        for i, syn in enumerate(synsets[:2], 1):\n",
    "            print(f\"  {i}. {syn.definition()}\")\n",
    "\n",
    "# ===== PART 2: N-GRAM LANGUAGE MODEL WITH LAPLACE SMOOTHING (10 marks) =====\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART 2: N-GRAM LANGUAGE MODEL WITH LAPLACE SMOOTHING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class NGramLanguageModel:\n",
    "    \"\"\"N-gram Language Model with Laplace Smoothing\"\"\"\n",
    "    \n",
    "    def __init__(self, n=2):\n",
    "        self.n = n\n",
    "        self.ngram_counts = defaultdict(int)\n",
    "        self.context_counts = defaultdict(int)\n",
    "        self.vocabulary = set()\n",
    "        \n",
    "    def train(self, text):\n",
    "        \"\"\"Train the n-gram model on text\"\"\"\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        \n",
    "        # Add to vocabulary\n",
    "        self.vocabulary.update(tokens)\n",
    "        \n",
    "        # Add padding\n",
    "        padded_tokens = ['<START>'] * (self.n - 1) + tokens + ['<END>']\n",
    "        self.vocabulary.add('<START>')\n",
    "        self.vocabulary.add('<END>')\n",
    "        \n",
    "        # Count n-grams\n",
    "        for i in range(len(padded_tokens) - self.n + 1):\n",
    "            ngram = tuple(padded_tokens[i:i + self.n])\n",
    "            context = ngram[:-1]\n",
    "            \n",
    "            self.ngram_counts[ngram] += 1\n",
    "            self.context_counts[context] += 1\n",
    "        \n",
    "        print(f\"\\nModel Training Complete!\")\n",
    "        print(f\"  Vocabulary size: {len(self.vocabulary)}\")\n",
    "        print(f\"  Total {self.n}-grams: {sum(self.ngram_counts.values())}\")\n",
    "        print(f\"  Unique {self.n}-grams: {len(self.ngram_counts)}\")\n",
    "    \n",
    "    def probability_with_smoothing(self, ngram):\n",
    "        \"\"\"Calculate probability with Laplace smoothing\"\"\"\n",
    "        ngram = tuple(ngram)\n",
    "        context = ngram[:-1]\n",
    "        \n",
    "        # Laplace smoothing: Add 1 to numerator and vocab size to denominator\n",
    "        vocab_size = len(self.vocabulary)\n",
    "        numerator = self.ngram_counts[ngram] + 1\n",
    "        denominator = self.context_counts[context] + vocab_size\n",
    "        \n",
    "        return numerator / denominator\n",
    "    \n",
    "    def probability_without_smoothing(self, ngram):\n",
    "        \"\"\"Calculate probability without smoothing\"\"\"\n",
    "        ngram = tuple(ngram)\n",
    "        context = ngram[:-1]\n",
    "        \n",
    "        if self.context_counts[context] == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return self.ngram_counts[ngram] / self.context_counts[context]\n",
    "    \n",
    "    def predict_next_word(self, context, top_k=5):\n",
    "        \"\"\"Predict next word given context\"\"\"\n",
    "        # Take only last (n-1) words as context\n",
    "        context = context[-(self.n-1):] if len(context) >= (self.n-1) else ['<START>'] * (self.n-1-len(context)) + context\n",
    "        context = tuple(context)\n",
    "        \n",
    "        # Calculate probabilities for all words\n",
    "        predictions = []\n",
    "        for word in self.vocabulary:\n",
    "            if word not in ['<START>', '<END>']:\n",
    "                ngram = context + (word,)\n",
    "                prob_smooth = self.probability_with_smoothing(ngram)\n",
    "                prob_no_smooth = self.probability_without_smoothing(ngram)\n",
    "                predictions.append((word, prob_smooth, prob_no_smooth))\n",
    "        \n",
    "        # Sort by probability (with smoothing)\n",
    "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return predictions[:top_k]\n",
    "\n",
    "# Training corpus (replace with exam question corpus)\n",
    "corpus = \"\"\"\n",
    "The cat sat on the mat. The dog sat on the log. \n",
    "The cat and the dog are friends. They play together every day.\n",
    "The mat is soft and the log is hard.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nTraining Corpus:\\n{corpus}\\n\")\n",
    "\n",
    "# Train bigram model (n=2)\n",
    "print(\"=\"*70)\n",
    "print(\"BIGRAM MODEL (n=2)\")\n",
    "print(\"=\"*70)\n",
    "bigram_model = NGramLanguageModel(n=2)\n",
    "bigram_model.train(corpus)\n",
    "\n",
    "# Predict next word\n",
    "context1 = ['the']\n",
    "print(f\"\\n--- Predictions after '{' '.join(context1)}' ---\")\n",
    "predictions = bigram_model.predict_next_word(context1, top_k=5)\n",
    "\n",
    "print(f\"\\n{'Word':<15} {'With Smoothing':<20} {'Without Smoothing':<20}\")\n",
    "print(\"-\" * 55)\n",
    "for word, prob_smooth, prob_no_smooth in predictions:\n",
    "    print(f\"{word:<15} {prob_smooth:<20.6f} {prob_no_smooth:<20.6f}\")\n",
    "\n",
    "# Train trigram model (n=3)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRIGRAM MODEL (n=3)\")\n",
    "print(\"=\"*70)\n",
    "trigram_model = NGramLanguageModel(n=3)\n",
    "trigram_model.train(corpus)\n",
    "\n",
    "context2 = ['the', 'cat']\n",
    "print(f\"\\n--- Predictions after '{' '.join(context2)}' ---\")\n",
    "predictions = trigram_model.predict_next_word(context2, top_k=5)\n",
    "\n",
    "print(f\"\\n{'Word':<15} {'With Smoothing':<20} {'Without Smoothing':<20}\")\n",
    "print(\"-\" * 55)\n",
    "for word, prob_smooth, prob_no_smooth in predictions:\n",
    "    print(f\"{word:<15} {prob_smooth:<20.6f} {prob_no_smooth:<20.6f}\")\n",
    "\n",
    "# Demonstrate smoothing effect\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON: IMPACT OF LAPLACE SMOOTHING\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nFor unseen n-gram ['the', 'zebra']:\")\n",
    "unseen_ngram = ['the', 'zebra']\n",
    "prob_with = bigram_model.probability_with_smoothing(unseen_ngram)\n",
    "prob_without = bigram_model.probability_without_smoothing(unseen_ngram)\n",
    "print(f\"  With Laplace Smoothing:    {prob_with:.6f}\")\n",
    "print(f\"  Without Smoothing:         {prob_without:.6f}\")\n",
    "print(f\"\\nSmoothing prevents zero probabilities for unseen n-grams!\")\n",
    "\n",
    "print(f\"\\n✓ Question 2 Complete!\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# QUESTION 3: INFORMATION EXTRACTION & SENTIMENT ANALYSIS (20 MARKS)\n",
    "# Task: 1. Implement NER to extract entities\n",
    "#       2. Perform Sentiment Analysis\n",
    "#       3. Implement Word Embeddings and visualize similarity\n",
    "# ============================================================================\n",
    "\n",
    "# -------- FIRST RUN THIS CELL (IMPORTS & SETUP) --------\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download NLTK data\n",
    "downloads = ['punkt', 'averaged_perceptron_tagger', 'maxent_ne_chunker', \n",
    "             'words', 'stopwords']\n",
    "for item in downloads:\n",
    "    nltk.download(item, quiet=True)\n",
    "\n",
    "print(\"✓ Setup Complete for Question 3!\")\n",
    "\n",
    "# -------- THEN RUN THIS CELL (MAIN CODE) --------\n",
    "\n",
    "# ===== PART 1: NAMED ENTITY RECOGNITION (5 marks) =====\n",
    "print(\"=\"*70)\n",
    "print(\"PART 1: NAMED ENTITY RECOGNITION (NER)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    \"\"\"Extract named entities from text\"\"\"\n",
    "    # Tokenize and POS tag\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # Named Entity Recognition\n",
    "    chunks = ne_chunk(pos_tags, binary=False)\n",
    "    \n",
    "    # Extract entities by type\n",
    "    entities = {\n",
    "        'PERSON': [],\n",
    "        'ORGANIZATION': [],\n",
    "        'GPE': [],  # Geo-Political Entity (locations)\n",
    "        'DATE': [],\n",
    "        'TIME': [],\n",
    "        'MONEY': [],\n",
    "        'FACILITY': []\n",
    "    }\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            entity_type = chunk.label()\n",
    "            entity_name = ' '.join(c[0] for c in chunk)\n",
    "            \n",
    "            if entity_type in entities:\n",
    "                entities[entity_type].append(entity_name)\n",
    "    \n",
    "    return entities, chunks\n",
    "\n",
    "# Test text for NER (replace with exam question text)\n",
    "ner_texts = [\n",
    "    \"Apple Inc. was founded by Steve Jobs in California. Tim Cook is the current CEO.\",\n",
    "    \"Microsoft CEO Satya Nadella announced new products in Seattle last Monday.\",\n",
    "    \"The Eiffel Tower in Paris attracts millions of tourists every year.\"\n",
    "]\n",
    "\n",
    "for i, text in enumerate(ner_texts, 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Text {i}: {text}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    entities, chunks = extract_named_entities(text)\n",
    "    \n",
    "    print(\"\\nExtracted Entities:\")\n",
    "    for entity_type, entity_list in entities.items():\n",
    "        if entity_list:\n",
    "            print(f\"  {entity_type}: {entity_list}\")\n",
    "    \n",
    "    # Show POS tags\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    print(f\"\\nPOS Tags (first 10): {pos_tags[:10]}\")\n",
    "\n",
    "# ===== PART 2: SENTIMENT ANALYSIS (5 marks) =====\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART 2: SENTIMENT ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    \"\"\"Lexicon-based sentiment analyzer\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Expanded sentiment lexicons\n",
    "        self.positive_words = {\n",
    "            'good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic',\n",
    "            'awesome', 'brilliant', 'outstanding', 'superb', 'perfect', 'love',\n",
    "            'loved', 'happy', 'joy', 'joyful', 'delighted', 'beautiful',\n",
    "            'best', 'better', 'positive', 'nice', 'fine', 'pleasant'\n",
    "        }\n",
    "        \n",
    "        self.negative_words = {\n",
    "            'bad', 'terrible', 'awful', 'horrible', 'poor', 'worst',\n",
    "            'hate', 'hated', 'sad', 'unhappy', 'disappointed', 'disappointing',\n",
    "            'negative', 'wrong', 'fail', 'failed', 'failure', 'worse',\n",
    "            'ugly', 'disgusting', 'boring', 'dull', 'unpleasant'\n",
    "        }\n",
    "        \n",
    "        # Intensifiers\n",
    "        self.intensifiers = {'very', 'extremely', 'really', 'absolutely', 'incredibly'}\n",
    "        \n",
    "        # Negations\n",
    "        self.negations = {'not', 'no', 'never', 'neither', \"n't\", 'nobody', 'nothing'}\n",
    "    \n",
    "    def analyze(self, text):\n",
    "        \"\"\"Analyze sentiment of text\"\"\"\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        \n",
    "        pos_score = 0\n",
    "        neg_score = 0\n",
    "        \n",
    "        for i, word in enumerate(tokens):\n",
    "            # Check for intensifiers\n",
    "            multiplier = 1.5 if i > 0 and tokens[i-1] in self.intensifiers else 1.0\n",
    "            \n",
    "            # Check for negations\n",
    "            is_negated = i > 0 and tokens[i-1] in self.negations\n",
    "            \n",
    "            # Score the word\n",
    "            if word in self.positive_words:\n",
    "                if is_negated:\n",
    "                    neg_score += 1 * multiplier\n",
    "                else:\n",
    "                    pos_score += 1 * multiplier\n",
    "            elif word in self.negative_words:\n",
    "                if is_negated:\n",
    "                    pos_score += 1 * multiplier\n",
    "                else:\n",
    "                    neg_score += 1 * multiplier\n",
    "        \n",
    "        # Determine overall sentiment\n",
    "        if pos_score > neg_score:\n",
    "            sentiment = 'Positive'\n",
    "            confidence = pos_score / (pos_score + neg_score) if (pos_score + neg_score) > 0 else 0\n",
    "        elif neg_score > pos_score:\n",
    "            sentiment = 'Negative'\n",
    "            confidence = neg_score / (pos_score + neg_score) if (pos_score + neg_score) > 0 else 0\n",
    "        else:\n",
    "            sentiment = 'Neutral'\n",
    "            confidence = 0.5\n",
    "        \n",
    "        return {\n",
    "            'sentiment': sentiment,\n",
    "            'positive_score': pos_score,\n",
    "            'negative_score': neg_score,\n",
    "            'confidence': confidence,\n",
    "            'total_words': len(tokens)\n",
    "        }\n",
    "\n",
    "# Test sentiment analysis\n",
    "sentiment_texts = [\n",
    "    \"This product is amazing and wonderful! I absolutely love it!\",\n",
    "    \"This is terrible and disappointing. I hate it.\",\n",
    "    \"The weather is okay today.\",\n",
    "    \"Not bad, but not great either.\",\n",
    "    \"This is not good at all. Very disappointing experience.\"\n",
    "]\n",
    "\n",
    "analyzer = SentimentAnalyzer()\n",
    "\n",
    "for i, text in enumerate(sentiment_texts, 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Text {i}: {text}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    result = analyzer.analyze(text)\n",
    "    \n",
    "    print(f\"\\nSentiment: {result['sentiment']}\")\n",
    "    print(f\"Positive Score: {result['positive_score']:.2f}\")\n",
    "    print(f\"Negative Score: {result['negative_score']:.2f}\")\n",
    "    print(f\"Confidence: {result['confidence']:.2%}\")\n",
    "\n",
    "# ===== PART 3: WORD EMBEDDINGS & SIMILARITY (10 marks) =====\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART 3: WORD EMBEDDINGS AND SIMILARITY VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class SimpleWordEmbeddings:\n",
    "    \"\"\"Simple co-occurrence based word embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size=2):\n",
    "        self.window_size = window_size\n",
    "        self.cooccurrence = defaultdict(lambda: defaultdict(int))\n",
    "        self.vocabulary = set()\n",
    "        self.word_to_vec = {}\n",
    "    \n",
    "    def train(self, text):\n",
    "        \"\"\"Build co-occurrence matrix from text\"\"\"\n",
    "        # Tokenize and clean\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [w for w in tokens if w.isalnum() and w not in stop_words]\n",
    "        \n",
    "        self.vocabulary.update(tokens)\n",
    "        \n",
    "        # Build co-occurrence matrix\n",
    "        for i, word in enumerate(tokens):\n",
    "            # Define window\n",
    "            start = max(0, i - self.window_size)\n",
    "            end = min(len(tokens), i + self.window_size + 1)\n",
    "            \n",
    "            # Count co-occurrences\n",
    "            for j in range(start, end):\n",
    "                if i != j:\n",
    "                    context_word = tokens[j]\n",
    "                    self.cooccurrence[word][context_word] += 1\n",
    "        \n",
    "        # Convert to vectors\n",
    "        all_context_words = sorted(self.vocabulary)\n",
    "        for word in self.vocabulary:\n",
    "            vector = [self.cooccurrence[word].get(context, 0) \n",
    "                     for context in all_context_words]\n",
    "            self.word_to_vec[word] = np.array(vector)\n",
    "        \n",
    "        print(f\"\\nEmbeddings Training Complete!\")\n",
    "        print(f\"  Vocabulary size: {len(self.vocabulary)}\")\n",
    "        print(f\"  Vector dimension: {len(all_context_words)}\")\n",
    "    \n",
    "    def cosine_similarity(self, word1, word2):\n",
    "        \"\"\"Calculate cosine similarity between two words\"\"\"\n",
    "        if word1 not in self.vocabulary or word2 not in self.vocabulary:\n",
    "            return 0.0\n",
    "        \n",
    "        vec1 = self.word_to_vec[word1]\n",
    "        vec2 = self.word_to_vec[word2]\n",
    "        \n",
    "        dot_product = np.dot(vec1, vec2)\n",
    "        norm1 = np.linalg.norm(vec1)\n",
    "        norm2 = np.linalg.norm(vec2)\n",
    "        \n",
    "        if norm1 == 0 or norm2 == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return dot_product / (norm1 * norm2)\n",
    "    \n",
    "    def most_similar(self, word, top_k=5):\n",
    "        \"\"\"Find most similar words\"\"\"\n",
    "        if word not in self.vocabulary:\n",
    "            return []\n",
    "        \n",
    "        similarities = []\n",
    "        for other_word in self.vocabulary:\n",
    "            if other_word != word:\n",
    "                sim = self.cosine_similarity(word, other_word)\n",
    "                similarities.append((other_word, sim))\n",
    "        \n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_k]\n",
    "    \n",
    "    def visualize_similarity_matrix(self, words):\n",
    "        \"\"\"Create similarity matrix for given words\"\"\"\n",
    "        n = len(words)\n",
    "        matrix = np.zeros((n, n))\n",
    "        \n",
    "        for i, word1 in enumerate(words):\n",
    "            for j, word2 in enumerate(words):\n",
    "                if word1 in self.vocabulary and word2 in self.vocabulary:\n",
    "                    matrix[i][j] = self.cosine_similarity(word1, word2)\n",
    "        \n",
    "        return matrix\n",
    "\n",
    "# Training corpus for embeddings\n",
    "embedding_corpus = \"\"\"\n",
    "The cat and dog are animals. Cats and dogs are pets.\n",
    "A cat is a small animal. A dog is a loyal animal.\n",
    "People love cats and dogs. Cats are independent. Dogs are friendly.\n",
    "The cat sleeps on the mat. The dog plays in the yard.\n",
    "Cats hunt mice. Dogs guard homes.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nTraining Corpus for Embeddings:\\n{embedding_corpus}\\n\")\n",
    "\n",
    "# Train embeddings\n",
    "embeddings = SimpleWordEmbeddings(window_size=2)\n",
    "embeddings.train(embedding_corpus)\n",
    "\n",
    "# Find similar words\n",
    "test_words = ['cat', 'dog', 'animal']\n",
    "\n",
    "for word in test_words:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Most similar words to '{word}':\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    similar = embeddings.most_similar(word, top_k=5)\n",
    "    \n",
    "    print(f\"\\n{'Rank':<6} {'Word':<15} {'Similarity':<15}\")\n",
    "    print(\"-\" * 40)\n",
    "    for rank, (similar_word, similarity) in enumerate(similar, 1):\n",
    "        print(f\"{rank:<6} {similar_word:<15} {similarity:.4f}\")\n",
    "\n",
    "# Similarity matrix visualization (text-based)\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"WORD SIMILARITY MATRIX\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "compare_words = ['cat', 'dog', 'animal', 'pet']\n",
    "similarity_matrix = embeddings.visualize_similarity_matrix(compare_words)\n",
    "\n",
    "# Print header\n",
    "print(f\"\\n{'Word':<12}\", end='')\n",
    "for word in compare_words:\n",
    "    print(f\"{word:<12}\", end='')\n",
    "print()\n",
    "print(\"-\" * (12 + 12 * len(compare_words)))\n",
    "\n",
    "# Print matrix\n",
    "for i, word1 in enumerate(compare_words):\n",
    "    print(f\"{word1:<12}\", end='')\n",
    "    for j, word2 in enumerate(compare_words):\n",
    "        print(f\"{similarity_matrix[i][j]:<12.4f}\", end='')\n",
    "    print()\n",
    "\n",
    "# Word pairs comparison\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SPECIFIC WORD PAIR SIMILARITIES\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "word_pairs = [\n",
    "    ('cat', 'dog'),\n",
    "    ('cat', 'animal'),\n",
    "    ('dog', 'animal'),\n",
    "    ('cat', 'pet')\n",
    "]\n",
    "\n",
    "for word1, word2 in word_pairs:\n",
    "    sim = embeddings.cosine_similarity(word1, word2)\n",
    "    print(f\"\\nSimilarity between '{word1}' and '{word2}': {sim:.4f}\")\n",
    "\n",
    "# Vector representation sample\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SAMPLE VECTOR REPRESENTATIONS\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "sample_word = 'cat'\n",
    "if sample_word in embeddings.vocabulary:\n",
    "    vector = embeddings.word_to_vec[sample_word]\n",
    "    print(f\"\\nVector for '{sample_word}' (first 10 dimensions):\")\n",
    "    print(vector[:10])\n",
    "    print(f\"Vector length: {len(vector)}\")\n",
    "    print(f\"Vector norm: {np.linalg.norm(vector):.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Question 3 Complete!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL QUESTIONS COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
