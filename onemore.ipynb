# ============================================================
# NLP EXAM PREPARATION - CORRECTED VERSION
# ============================================================

# ============================================================
# INSTALLATION GUIDE
# ============================================================
"""
# Required Libraries Installation:
pip install nltk
pip install scikit-learn
pip install gensim
pip install numpy
pip install pandas

# NLTK Data Downloads (run once):
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')
nltk.download('vader_lexicon')
"""

# ============================================================
# Q1: TEXT PREPROCESSING & REPRESENTATION
# ============================================================

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# Download NLTK resources (run only once)
try:
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
except:
    print("Error downloading NLTK data. Please check your internet connection.")

# Example text (can change in exam)
texts = ["Natural Language Processing is amazing!",
         "Language helps humans communicate effectively."]

# --- Step 1: Preprocess text ---
try:
    stop_words = set(stopwords.words('english'))
    clean_texts = []
    
    for text in texts:
        tokens = word_tokenize(text.lower())
        filtered = [w for w in tokens if w.isalpha() and w not in stop_words]
        clean_texts.append(" ".join(filtered))
    
    print("Cleaned Texts:", clean_texts)
except LookupError:
    print("Error: NLTK data not found. Run nltk.download('punkt') and nltk.download('stopwords')")

# --- Step 2: Bag of Words Representation ---
try:
    bow = CountVectorizer()
    bow_matrix = bow.fit_transform(clean_texts)
    print("\nBag of Words Matrix:\n", bow_matrix.toarray())
    print("BOW Features:", bow.get_feature_names_out())
except ValueError as e:
    print(f"Error in BOW: {e}. Ensure texts are not empty.")

# --- Step 3: TF-IDF Representation ---
try:
    tfidf = TfidfVectorizer()
    tfidf_matrix = tfidf.fit_transform(clean_texts)
    print("\nTF-IDF Matrix:\n", tfidf_matrix.toarray())
    print("TF-IDF Features:", tfidf.get_feature_names_out())
except ValueError as e:
    print(f"Error in TF-IDF: {e}")

"""
POSSIBLE ERRORS & FIXES (Q1):
1. LookupError: Missing punkt/stopwords → run nltk.download('punkt') and nltk.download('stopwords')
2. ValueError: Empty vocabulary → ensure texts are not empty or all stopwords
3. Non-English text → may need different tokenizer or stopwords list
"""

# ============================================================
# Q2: SEMANTIC UNDERSTANDING & LANGUAGE MODELING
# ============================================================

from nltk.corpus import wordnet
from collections import Counter

try:
    nltk.download('wordnet', quiet=True)
    nltk.download('omw-1.4', quiet=True)
except:
    print("Error downloading WordNet data.")

# --- Step 1: Synonyms, Antonyms, Hypernyms ---
word = "good"
synonyms, antonyms, hypernyms = set(), set(), set()

try:
    for syn in wordnet.synsets(word):
        for lemma in syn.lemmas():
            synonyms.add(lemma.name())
            if lemma.antonyms():
                antonyms.add(lemma.antonyms()[0].name())
        for hypernym in syn.hypernyms():
            for lemma in hypernym.lemmas():
                hypernyms.add(lemma.name())

    print(f"\n--- Word: '{word}' ---")
    print("Synonyms:", synonyms)
    print("Antonyms:", antonyms)
    print("Hypernyms:", hypernyms)
except Exception as e:
    print(f"Error in WordNet lookup: {e}")

# --- Step 2: N-Gram Language Model (Bigram + Laplace smoothing) ---
text = "the cat sat on the mat"
tokens = text.split()

try:
    bigrams = [(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]
    freq = Counter(bigrams)
    vocab = set(tokens)

    def laplace_prob(w1, w2):
        """Calculate bigram probability with Laplace smoothing"""
        count_w1 = tokens.count(w1)
        if count_w1 == 0:
            return 0
        return (freq[(w1, w2)] + 1) / (count_w1 + len(vocab))

    print("\n--- Bigram Language Model ---")
    print(f"P('sat' | 'cat') = {laplace_prob('cat', 'sat'):.4f}")
    print(f"P('on' | 'sat') = {laplace_prob('sat', 'on'):.4f}")
    print(f"Bigram frequencies: {dict(freq)}")
except IndexError:
    print("Error: Text too short for bigram model.")
except ZeroDivisionError:
    print("Error: Word not found in text.")

"""
POSSIBLE ERRORS & FIXES (Q2):
1. LookupError: wordnet not found → nltk.download('wordnet')
2. ZeroDivisionError → check if word exists in tokens
3. IndexError → ensure text has at least 2 words
4. Use lowercase tokens to match word counts properly
"""

# ============================================================
# Q3: INFORMATION EXTRACTION & SENTIMENT ANALYSIS
# ============================================================

from nltk import pos_tag, ne_chunk
from nltk.sentiment import SentimentIntensityAnalyzer
from gensim.models import Word2Vec

try:
    nltk.download('averaged_perceptron_tagger', quiet=True)
    nltk.download('maxent_ne_chunker', quiet=True)
    nltk.download('words', quiet=True)
    nltk.download('vader_lexicon', quiet=True)
except:
    print("Error downloading NLTK models.")

# --- Step 1: Named Entity Recognition (NER) ---
text = "Elon Musk founded SpaceX in California."

try:
    tokens = word_tokenize(text)
    tags = pos_tag(tokens)
    ner_tree = ne_chunk(tags)
    
    print("\n--- Named Entity Recognition ---")
    print("Text:", text)
    print("Named Entities:")
    print(ner_tree)
    
    # Extract named entities
    entities = []
    for chunk in ner_tree:
        if hasattr(chunk, 'label'):
            entities.append((chunk.label(), ' '.join(c[0] for c in chunk)))
    print("Extracted Entities:", entities)
except Exception as e:
    print(f"NER Error: {e}")

# --- Step 2: Sentiment Analysis ---
try:
    sia = SentimentIntensityAnalyzer()
    test_sentence = "The product is great but expensive."
    sentiment = sia.polarity_scores(test_sentence)
    
    print("\n--- Sentiment Analysis ---")
    print("Text:", test_sentence)
    print("Sentiment Scores:", sentiment)
    print(f"Overall: {'Positive' if sentiment['compound'] > 0 else 'Negative' if sentiment['compound'] < 0 else 'Neutral'}")
except Exception as e:
    print(f"Sentiment Analysis Error: {e}")

# --- Step 3: Word Embeddings (Word2Vec Similarity) ---
sentences = [["nlp", "is", "fun"],
             ["deep", "learning", "is", "powerful"],
             ["language", "models", "learn", "patterns"],
             ["nlp", "uses", "machine", "learning"]]

try:
    model = Word2Vec(sentences, vector_size=30, min_count=1, seed=42)
    
    print("\n--- Word2Vec Embeddings ---")
    print(f"Similarity (nlp, learning): {model.wv.similarity('nlp', 'learning'):.4f}")
    print(f"Most similar to 'language': {model.wv.most_similar('language', topn=3)}")
    print(f"Vocabulary size: {len(model.wv)}")
except KeyError as e:
    print(f"Word not in vocabulary: {e}")
except Exception as e:
    print(f"Word2Vec Error: {e}")

"""
POSSIBLE ERRORS & FIXES (Q3):
1. LookupError: missing nltk data → run nltk.download() for required modules
2. ModuleNotFoundError: gensim not found → pip install gensim
3. KeyError in similarity → check if word exists in vocabulary using 'word' in model.wv
4. For visualization: use matplotlib (not included here but can be added)
"""

print("\n" + "="*60)
print("EXAM PREPARATION COMPLETE!")
print("="*60)
