{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52b1da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Q1: TEXT PREPROCESSING & REPRESENTATION\n",
    "# ============================================================\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Download NLTK resources (run only once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Example text (can change in exam)\n",
    "texts = [\"Natural Language Processing is amazing!\",\n",
    "         \"Language helps humans communicate effectively.\"]\n",
    "\n",
    "# --- Step 1: Preprocess text ---\n",
    "stop_words = set(stopwords.words('english'))\n",
    "clean_texts = []\n",
    "for text in texts:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered = [w for w in tokens if w.isalpha() and w not in stop_words]\n",
    "    clean_texts.append(\" \".join(filtered))\n",
    "print(\"Cleaned Texts:\", clean_texts)\n",
    "\n",
    "# --- Step 2: Bag of Words Representation ---\n",
    "bow = CountVectorizer()\n",
    "bow_matrix = bow.fit_transform(clean_texts)\n",
    "print(\"\\nBag of Words Matrix:\\n\", bow_matrix.toarray())\n",
    "print(\"BOW Features:\", bow.get_feature_names_out())\n",
    "\n",
    "# --- Step 3: TF-IDF Representation ---\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(clean_texts)\n",
    "print(\"\\nTF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n",
    "print(\"TF-IDF Features:\", tfidf.get_feature_names_out())\n",
    "\n",
    "\"\"\"\n",
    "# POSSIBLE ERRORS & FIXES (Q1)\n",
    "1. LookupError: Missing punkt/stopwords → run nltk.download('punkt') and nltk.download('stopwords')\n",
    "2. ValueError: Empty vocabulary → ensure texts are not empty or all stopwords\n",
    "3. Non-English text → may need different tokenizer\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# Q2: SEMANTIC UNDERSTANDING & LANGUAGE MODELING\n",
    "# ============================================================\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# --- Step 1: Synonyms, Antonyms, Hypernyms ---\n",
    "word = \"good\"\n",
    "synonyms, antonyms, hypernyms = set(), set(), set()\n",
    "\n",
    "for syn in wordnet.synsets(word):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.add(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.add(l.antonyms()[0].name())\n",
    "    for h in syn.hypernyms():\n",
    "        for l in h.lemmas():\n",
    "            hypernyms.add(l.name())\n",
    "\n",
    "print(\"\\nSynonyms:\", synonyms)\n",
    "print(\"Antonyms:\", antonyms)\n",
    "print(\"Hypernyms:\", hypernyms)\n",
    "\n",
    "# --- Step 2: N-Gram Language Model (Bigram + Laplace smoothing) ---\n",
    "text = \"the cat sat on the mat\"\n",
    "tokens = text.split()\n",
    "bigrams = [(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]\n",
    "freq = Counter(bigrams)\n",
    "vocab = set(tokens)\n",
    "\n",
    "def laplace_prob(w1, w2):\n",
    "    return (freq[(w1, w2)] + 1) / (tokens.count(w1) + len(vocab))\n",
    "\n",
    "print(\"\\nP('sat' | 'cat') =\", laplace_prob('cat', 'sat'))\n",
    "print(\"P('on' | 'mat') =\", laplace_prob('mat', 'on'))\n",
    "\n",
    "\"\"\"\n",
    "# POSSIBLE ERRORS & FIXES (Q2)\n",
    "1. LookupError: wordnet not found → nltk.download('wordnet')\n",
    "2. ZeroDivisionError → if word not found in tokens\n",
    "3. IndexError → ensure text has enough words\n",
    "4. Use lowercase tokens to match word counts properly\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# Q3: INFORMATION EXTRACTION & SENTIMENT ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "import gensim\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# --- Step 1: Named Entity Recognition (NER) ---\n",
    "text = \"Elon Musk founded SpaceX in California.\"\n",
    "tokens = word_tokenize(text)\n",
    "tags = pos_tag(tokens)\n",
    "ner_tree = ne_chunk(tags)\n",
    "print(\"\\nNamed Entities:\\n\", ner_tree)\n",
    "\n",
    "# --- Step 2: Sentiment Analysis ---\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sentiment = sia.polarity_scores(\"The product is great but expensive.\")\n",
    "print(\"\\nSentiment Scores:\", sentiment)\n",
    "\n",
    "# --- Step 3: Word Embeddings (Word2Vec Similarity) ---\n",
    "sentences = [[\"nlp\", \"is\", \"fun\"],\n",
    "             [\"deep\", \"learning\", \"is\", \"powerful\"],\n",
    "             [\"language\", \"models\", \"learn\", \"patterns\"]]\n",
    "\n",
    "model = Word2Vec(sentences, vector_size=30, min_count=1)\n",
    "print(\"\\nSimilarity (nlp, learning):\", model.wv.similarity('nlp', 'learning'))\n",
    "print(\"Most similar to 'language':\", model.wv.most_similar('language'))\n",
    "\n",
    "\"\"\"\n",
    "# POSSIBLE ERRORS & FIXES (Q3)\n",
    "1. LookupError: missing nltk data → run nltk.download() for required modules\n",
    "2. ModuleNotFoundError: gensim not found → pip install gensim\n",
    "3. KeyError in similarity → check if word exists in vocabulary\n",
    "4. Matplotlib error → if visualization required, use %matplotlib inline\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5d5b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Q1: TEXT PREPROCESSING & REPRESENTATION\n",
    "# ============================================================\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# --- Predicted Exam Question ---\n",
    "# Q1: Write a program to preprocess text (tokenize, remove stopwords, lowercase)\n",
    "# and represent it using Bag-of-Words and TF-IDF. Compare both.\n",
    "\n",
    "texts = [\"Natural Language Processing is amazing!\",\n",
    "         \"Language helps humans communicate effectively.\"]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "clean_texts = []\n",
    "\n",
    "for text in texts:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered = [w for w in tokens if w.isalpha() and w not in stop_words]\n",
    "    clean_texts.append(\" \".join(filtered))\n",
    "\n",
    "print(\"Preprocessed Texts:\", clean_texts)\n",
    "\n",
    "# Bag of Words\n",
    "bow = CountVectorizer()\n",
    "bow_matrix = bow.fit_transform(clean_texts)\n",
    "print(\"\\nBag of Words:\\n\", bow_matrix.toarray())\n",
    "\n",
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(clean_texts)\n",
    "print(\"\\nTF-IDF:\\n\", tfidf_matrix.toarray())\n",
    "\n",
    "\"\"\"\n",
    "# Possible Errors (Q1)\n",
    "1. LookupError: missing punkt/stopwords → run nltk.download('punkt'), nltk.download('stopwords')\n",
    "2. ValueError: empty vocabulary → ensure input text not empty\n",
    "3. UnicodeDecodeError → ensure text is plain English\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# Q2: SEMANTIC UNDERSTANDING & LANGUAGE MODELING\n",
    "# ============================================================\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# --- Predicted Exam Question ---\n",
    "# Q2: Write a program to find synonyms, antonyms, and hypernyms of a given word.\n",
    "# Then implement a Bigram Language Model with Laplace smoothing.\n",
    "\n",
    "word = \"happy\"\n",
    "synonyms, antonyms, hypernyms = set(), set(), set()\n",
    "\n",
    "for syn in wordnet.synsets(word):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.add(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.add(l.antonyms()[0].name())\n",
    "    for h in syn.hypernyms():\n",
    "        for l in h.lemmas():\n",
    "            hypernyms.add(l.name())\n",
    "\n",
    "print(\"\\nSynonyms:\", synonyms)\n",
    "print(\"Antonyms:\", antonyms)\n",
    "print(\"Hypernyms:\", hypernyms)\n",
    "\n",
    "# Bigram Model (with Laplace smoothing)\n",
    "text = \"the cat sat on the mat\"\n",
    "tokens = text.split()\n",
    "bigrams = [(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]\n",
    "freq = Counter(bigrams)\n",
    "vocab = set(tokens)\n",
    "\n",
    "def laplace_prob(w1, w2):\n",
    "    return (freq[(w1, w2)] + 1) / (tokens.count(w1) + len(vocab))\n",
    "\n",
    "print(\"\\nP('sat' | 'cat') =\", laplace_prob('cat', 'sat'))\n",
    "print(\"P('on' | 'mat') =\", laplace_prob('mat', 'on'))\n",
    "\n",
    "\"\"\"\n",
    "# Possible Errors (Q2)\n",
    "1. LookupError: wordnet not found → run nltk.download('wordnet')\n",
    "2. ZeroDivisionError → check token existence\n",
    "3. IndexError → if text too short for bigrams\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# Q3: INFORMATION EXTRACTION & SENTIMENT ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "import gensim\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# --- Predicted Exam Question ---\n",
    "# Q3: Write a program to extract named entities from a text,\n",
    "# perform sentiment analysis, and visualize word similarities using Word2Vec.\n",
    "\n",
    "text = \"Elon Musk founded SpaceX in California.\"\n",
    "tokens = word_tokenize(text)\n",
    "tags = pos_tag(tokens)\n",
    "ner = ne_chunk(tags)\n",
    "print(\"\\nNamed Entities:\\n\", ner)\n",
    "\n",
    "# Sentiment Analysis\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sentence = \"The movie was great but a bit too long.\"\n",
    "print(\"\\nSentiment Scores:\", sia.polarity_scores(sentence))\n",
    "\n",
    "# Word2Vec Similarity\n",
    "sentences = [[\"nlp\", \"is\", \"fun\"],\n",
    "             [\"machine\", \"learning\", \"is\", \"powerful\"],\n",
    "             [\"language\", \"models\", \"learn\", \"patterns\"]]\n",
    "\n",
    "model = Word2Vec(sentences, vector_size=30, min_count=1)\n",
    "print(\"\\nSimilarity (nlp, learning):\", model.wv.similarity('nlp', 'learning'))\n",
    "print(\"Most similar to 'language':\", model.wv.most_similar('language'))\n",
    "\n",
    "\"\"\"\n",
    "# Possible Errors (Q3)\n",
    "1. LookupError: missing nltk models → download required resources\n",
    "2. gensim not installed → pip install gensim\n",
    "3. KeyError: if word not in vocabulary\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
