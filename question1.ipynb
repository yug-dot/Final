{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b282fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Q1: Text Preprocessing & Representation ---\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# download only once\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# sample text (can be changed in exam)\n",
    "texts = [\"Natural Language Processing is fun!\",\n",
    "         \"Language is a key to communication.\"]\n",
    "\n",
    "# 1. Preprocess text\n",
    "stop_words = set(stopwords.words('english'))\n",
    "clean_texts = []\n",
    "for t in texts:\n",
    "    tokens = word_tokenize(t.lower())\n",
    "    filtered = [w for w in tokens if w.isalpha() and w not in stop_words]\n",
    "    clean_texts.append(\" \".join(filtered))\n",
    "print(\"Preprocessed:\", clean_texts)\n",
    "\n",
    "# 2. Bag of Words\n",
    "bow = CountVectorizer()\n",
    "bow_matrix = bow.fit_transform(clean_texts)\n",
    "print(\"\\nBOW:\\n\", bow_matrix.toarray())\n",
    "\n",
    "# 3. TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(clean_texts)\n",
    "print(\"\\nTF-IDF:\\n\", tfidf_matrix.toarray())\n",
    "\n",
    "\"\"\"\n",
    "Possible Errors & Fixes:\n",
    "1. LookupError: punkt/stopwords not found → run nltk.download('punkt'), nltk.download('stopwords')\n",
    "2. ValueError: empty vocabulary → ensure texts not empty\n",
    "3. UnicodeDecodeError → use plain English or UTF-8 strings\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69294ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Q1: Text Preprocessing & Representation ---\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Download dependencies (only first time)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample text\n",
    "text1 = \"Natural Language Processing is amazing!\"\n",
    "text2 = \"Language Processing with Python is powerful.\"\n",
    "\n",
    "# 1. Text Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    clean_tokens = [w for w in tokens if w.isalpha() and w not in stop_words]\n",
    "    return ' '.join(clean_tokens)\n",
    "\n",
    "clean_texts = [preprocess(text1), preprocess(text2)]\n",
    "print(\"Preprocessed Texts:\", clean_texts)\n",
    "\n",
    "# 2. Representation - Bag of Words\n",
    "bow_vectorizer = CountVectorizer()\n",
    "bow_matrix = bow_vectorizer.fit_transform(clean_texts)\n",
    "print(\"\\nBag of Words Representation:\\n\", bow_matrix.toarray())\n",
    "\n",
    "# 3. Representation - TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(clean_texts)\n",
    "print(\"\\nTF-IDF Representation:\\n\", tfidf_matrix.toarray())\n",
    "\n",
    "# Comparison\n",
    "print(\"\\nFeature Names:\", bow_vectorizer.get_feature_names_out())\n",
    "\n",
    "\"\"\"\n",
    "Possible Errors & Fixes:\n",
    "1. LookupError: 'punkt' not found → Run nltk.download('punkt')\n",
    "2. LookupError: 'stopwords' not found → Run nltk.download('stopwords')\n",
    "3. ValueError: Empty vocabulary → Ensure text isn't empty or all stopwords.\n",
    "4. Encoding errors in text → Use UTF-8 encoding or str() conversion.\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
